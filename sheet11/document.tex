\documentclass[a4paper]{article}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\pagestyle{plain}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{eurosym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{dlfltxbcodetips}
\usepackage{cancel}
\usepackage{bbm}
\usepackage{graphicx}

%    \usepackage{tkz-graph}  
%    \usetikzlibrary{shapes.geometric}%   
% 	 \usetikzlibrary{bayesnet}
	
\input{preamble.tex} % import config
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}  
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\vivid}{\stackrel{\text{vivid}}{=}}
\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand\indep{\protect\mathpalette{\protect\indeP}{\perp}}
\def\indeP#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\mlq}{F(\q)}
\newcommand{\si}{s}
\newcommand{\sj}{{s'}}
\newcommand{\p}{p}
\newcommand{\q}{r}
\newcommand{\z}{w}
\newcommand{\e}{\textbf{e}}
%\newcommand{\x}{\mathcal{D}}
%\newcommand{\mlq}{-L(\q)}
%\newcommand{\si}{i}
%\newcommand{\sj}{j}
%\newcommand{\p}{p}
%\newcommand{\q}{q}
%\newcommand{\z}{Z}
%\newcommand{\x}{X}

\newcommand{\pxgz}{\p(\x|\z)}
\newcommand{\pzgx}{\p(\z|\x)}
\newcommand{\pxz}{\p(\x,\z)}
\newcommand{\px}{\p(\x)}

\newcommand{\qz}{\q(\z)}
\newcommand{\qi}{\q_\si}
\newcommand{\qj}{\q_\sj}
\newcommand{\qk}{\q_k}
\newcommand{\pz}{\p(\z)}
\newcommand{\pzi}{\p(\z_\si)}
\newcommand{\pzj}{\p(\z_\sj)}
\newcommand{\pzk}{\p(\z_k)}

\newcommand{\sigm}{\text{sigm}}

\newcommand{\zi}{\z_\si}
\newcommand{\zj}{\z_\sj}
\newcommand{\zk}{\z_k}
\newcommand{\dz}{\mathrm{d}\z}
\newcommand{\dzi}{\mathrm{d}\z_\si}
\newcommand{\dzj}{\mathrm{d}\z_\sj}
\newcommand{\dzk}{\mathrm{d}\z_k} 

\newcommand{\E}{\mathbb{E}}

\newcommand{\xt}{x^\top}



%\newcommand{\propq}{ \underset{ \text{wrt.}\put(1,1){\scriptsize *}q }{\propto} }

\newcommand{\eqs}[1]{ \stackrel{#1}{=}  }
\newcommand{\eqq}{  \overset{\text{!}}{=} }
\newcommand{\propqz}{ \underset{ \overset{\text{wrt.}}{\qz} }{\propto} }
\newcommand{\propz}{ \underset{ \overset{\text{wrt.}}{\z} }{\propto} }
\newcommand{\propzj}{ \underset{ \overset{\text{wrt.}}{\zj} }{\propto} }
%\newcommand{\constw}{ \underset{ \text{ wrt. \w} }{\text{ const}} }
%\newcommand{\consty}{ \underset{ \text{ wrt. \y} }{\text{ const}} }
\newcommand{\const}[1]{{\underset{ \text{ wrt. #1} }{\text{ const}} }}

\newcommand{\tr}[1]{{#1^\top}}
\newcommand{\inv}[1]{{#1^{-1}}}
\newcommand{\trk}[1]{{(#1)^\top}}
\newcommand{\invk}[1]{{(#1)^{-1}}}
\newcommand{\tin}[1]{{(#1^\top)^{-1}}}

\newcommand{\Yt}{{Y^\top}}
\newcommand{\Xt}{{X^\top}}
\newcommand{\yt}{{\y^\top}}
\newcommand{\Wt}{{\W^\top}}
\newcommand{\w}{w}
\newcommand{\lx}{{\lambda_{x}}}
\newcommand{\ly}{{\lambda_{y}}}
\newcommand{\rhi}{{r^{(i)}}}
\newcommand{\rhj}{{r^{(j)}}}
\newcommand{\xhi}{{x^{(i)}}}
\newcommand{\shi}{{s^{(i)}}}
\newcommand{\xhj}{{x^{(j)}}}
\newcommand{\shj}{{s^{(j)}}}
\newcommand{\Wi}{{W_{i}}}
\newcommand{\Wj}{{W_{j}}}
\newcommand{\Wit}{W_{i}^\top}
\newcommand{\Wjt}{W_{j}^\top}
\newcommand{\Wij}{W_{ij}}
\newcommand{\Wlk}{W_{lk}}
\newcommand{\ax}{{\alpha_{x}}}
\newcommand{\ay}{{\alpha_{y}}}
\newcommand{\axt}{\alpha_{x}^\top}
\newcommand{\ayt}{\alpha_{y}^\top}
\newcommand{\Cxx}{C_{xx}}
\newcommand{\Cxy}{C_{xy}}
\newcommand{\Cyx}{C_{yx}} 
\newcommand{\Cyy}{C_{yy}}
\newcommand{\1}{\mathds{1}}
\newcommand{\lag}{\mathcal{L}}
\newcommand\bigforall{\mbox{\Large $\mathsurround0pt\forall$}} 

\begin{document}

% header configuration
\title{\b{Exercise Sheet 11}}
\author{Machine Learning 2, SS16}

\maketitle

% authors
Mario Tambos, 380599;\quad Viktor Jeney, 348969;\quad Sascha Huk, 321249;\quad Jan Tinapp, 0380549\\

\extitle{Exercise 1}
The Lagrangian of the given primal problem is: 
\begin{gather*}
	\lag(R,c,\xi,\alpha,\lambda) = R^2 + {1\over n \nu}\sum_i \xi_i
	- \sum_i \alpha_i ( R^2 + \xi_i - \norm{\phi(x_i)-c}  )
	- \sum_i \lambda_i \xi_i
	\\
	=
	R^2 + {1\over n\nu}\sum_i \xi_i
	- \sum_i \alpha_i ( R^2 + \xi_i - (\phi(x_i)^\top\phi(x_i) + c^\top c - 2c^\top\phi(x_i))  )
	- \sum_i \lambda_i \xi_i
	\\
	=
	R^2 + {1\over n\nu}\sum_i \xi_i
	- \sum_i \alpha_i ( R^2 + \xi_i - \phi(x_i)^\top\phi(x_i) - c^\top c + 2c^\top\phi(x_i) )
	- \sum_i \lambda_i \xi_i	
\end{gather*}
We now differentiate w.r.t. primal variables $R,c,\xi$: 
\begin{gather*}
	{\partial\over\partial R}\lag(R,c,\xi,\alpha,\lambda) 
	=
	2R-2R\sum_i\alpha_i \eqq 0 
	\quad\quad\Longrightarrow\quad\quad
	\sum_i \alpha_i = 1
	\\
	{\partial\over\partial c}\lag(R,c,\xi,\alpha,\lambda) 
	=
	2c\sum_i\alpha_i - 2\sum_i\alpha_i\phi(x_i) \eqq 0
	\quad\quad\Longrightarrow\quad\quad
	c = \sum_i\alpha_i\phi(x_i) 
	\\
	{\partial\over\partial \xi_i}\lag(R,c,\xi,\alpha,\lambda) 
	=
	{1\over n\nu} - \alpha_i - \lambda_i \eqq 0
	\quad\quad\Longrightarrow\quad\quad
	{1\over n\nu} = \alpha_i + \lambda_i
\end{gather*}
The Lagrangian of the dual problem then can be obtained by plugging in the derived results: 
\begin{gather*}
	\lag(\alpha, \lambda )
	=
	R^2 + {1\over n\nu}\sum_i \xi_i
	- \sum_i \alpha_i ( R^2 + \xi_i - \phi(x_i)^\top\phi(x_i) - c^\top c + 2c^\top\phi(x_i) )
	- \sum_i \lambda_i \xi_i
	\\
	=
	\cancel{R^2} + {1\over n\nu}\sum_i \xi_i
	- \cancel{R^2} \underbrace{\cancel{\sum_i \alpha_i}}_{1}
	- \sum_i \alpha_i \xi_i
	+ \sum_i \alpha_i k(x_i,x_i)
	+ c^\top c \underbrace{\sum_i \alpha_i}_{1} 
	- 2c^\top \sum_i \alpha_i \phi(x_i)
	- \sum_i \lambda_i \xi_i
	\\
	=
	\cancel{{1\over n\nu}\sum_i \xi_i}
	- \cancel{\sum_i \underbrace{(\alpha_i + \lambda_i)}_{1\over n\nu} \xi_i}
	+ \sum_i \alpha_i k(x_i,x_i)
	+ c^\top c 
	- 2c^\top\underbrace{\sum_i \alpha_i \phi(x_i)}_{c}
	\\
	=
	\sum_i \alpha_i k(x_i,x_i)
	- c^\top c 
\end{gather*}
Lastly, by using the definition of c, we obtain the dual program:
\begin{gather*}
	\max_\alpha \sum_i \alpha_i k(x_i,x_i) - \sum_i\sum_j\alpha_i\alpha_j k(x_i,x_j)
	\\
	%\quad\quad\text{s.t.} \quad\quad
	\text{s.t.} \quad\quad
	\sum_i\alpha_i = 1
	\quad\text{and} \quad
	\alpha_i \geq 0
	\quad\text{and} \quad
	\lambda_i \geq 0
	\quad\text{and} \quad
	\alpha_i + \lambda_i = {1\over n\nu}
	\\
	\Longrightarrow
	\max_\alpha \sum_i \alpha_i k(x_i,x_i) - \sum_i\sum_j\alpha_i\alpha_j k(x_i,x_j)
	\quad\quad\text{s.t.} \quad\quad
	\sum_i\alpha_i = 1
	\quad\text{and} \quad
	{1\over n\nu} \geq \alpha_i \geq 0
\end{gather*}
The primal variable c is determined by $c=\sum_i\alpha_i\phi(x_i)$.
R, the radius, then can be found by using the constraint of the primal problem 
(i.e. using the support vectors on the gutter to solve $\norm{\phi(x_i)-c} = R$).
\newpage
\extitle{Exercise 2}
Form of the QP problem:
\begin{align*}
	\min_\alpha \alpha^\top P\alpha + q^\top \alpha \quad\quad & \text{s.t. }\quad G\alpha\preceq h
	 \quad \text{ and }\quad A\alpha=b  
\end{align*}
The objective of the dual problem can be further derived like this:
\begin{gather*}
	\max_\alpha\lag(\alpha )
	=
	\max_\alpha \sum_i \alpha_i k(x_i,x_i) - \sum_i\sum_j\alpha_i\alpha_j k(x_i,x_j)
	\quad\quad\text{s.t.} \quad\quad
	\sum_i\alpha_i = 1
	\quad\text{and} \quad
	{1\over n\nu} \geq \alpha_i \geq 0
	\\
	\Longrightarrow
	\min_\alpha -\lag(\alpha )
	=
	\min_\alpha \sum_i\sum_j\alpha_i\alpha_j k(x_i,x_j) - \sum_i \alpha_i k(x_i,x_i) 
	\quad\quad\text{s.t.} \quad\quad
	\sum_i\alpha_i = 1
	\quad\text{and} \quad
	{1\over n\nu} \geq \alpha_i \geq 0	
\end{gather*}
The variables can now be identified as follows:
\begin{align*}
	& P\hat{=}(k(x_i,x_j))_{i,j}\in\mathbb{R}^{n\times n} & \quad & 
	q\hat{=}-(k(x_i,x_i))_{i}\in\mathbb{R}^{n} & \quad & 
	G\hat{=} 
	\begin{bmatrix}
		\textbf{I}_n \\ 
		\textbf{-I}_n 
	\end{bmatrix}\in\mathbb{R}^{2n\times n}
	\\
	&h\hat{=}{1\over n\nu}
	\begin{bmatrix}
		\mathbbm{\vec{1}}_n \\ 
		\vec{0}_n 
	\end{bmatrix}\in\mathbb{R}^{2n}
	& \quad & A\hat{=}
	\begin{bmatrix}
		1 & \ldots & 1 \\ 
		\phi(x_1) & \ldots & \phi(x_n)  
	\end{bmatrix}\in\mathbb{R}^{(d+1)\times n}
	& \quad & b\hat{=}
	\begin{bmatrix}
		1 \\ 
		\vec{c} 
	\end{bmatrix}\in\mathbb{R}^{d+1}
\end{align*} 
\end{document}






