\documentclass[a4paper]{article}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\pagestyle{plain}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{eurosym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{dlfltxbcodetips}
\usepackage{cancel}

%    \usepackage{tkz-graph}  
%    \usetikzlibrary{shapes.geometric}%   
% 	 \usetikzlibrary{bayesnet}
	
\input{preamble.tex} % import config
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}  
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\vivid}{\stackrel{\text{vivid}}{=}}
\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand\indep{\protect\mathpalette{\protect\indeP}{\perp}}
\def\indeP#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\mlq}{F(\q)}
\newcommand{\si}{s}
\newcommand{\sj}{{s'}}
\newcommand{\p}{p}
\newcommand{\q}{r}
\newcommand{\z}{w}
\newcommand{\x}{\mathcal{D}}
%\newcommand{\mlq}{-L(\q)}
%\newcommand{\si}{i}
%\newcommand{\sj}{j}
%\newcommand{\p}{p}
%\newcommand{\q}{q}
%\newcommand{\z}{Z}
%\newcommand{\x}{X}

\newcommand{\pxgz}{\p(\x|\z)}
\newcommand{\pzgx}{\p(\z|\x)}
\newcommand{\pxz}{\p(\x,\z)}
\newcommand{\px}{\p(\x)}

\newcommand{\qz}{\q(\z)}
\newcommand{\qi}{\q_\si}
\newcommand{\qj}{\q_\sj}
\newcommand{\qk}{\q_k}
\newcommand{\pz}{\p(\z)}
\newcommand{\pzi}{\p(\z_\si)}
\newcommand{\pzj}{\p(\z_\sj)}
\newcommand{\pzk}{\p(\z_k)}



\newcommand{\zi}{\z_\si}
\newcommand{\zj}{\z_\sj}
\newcommand{\zk}{\z_k}
\newcommand{\dz}{\mathrm{d}\z}
\newcommand{\dzi}{\mathrm{d}\z_\si}
\newcommand{\dzj}{\mathrm{d}\z_\sj}
\newcommand{\dzk}{\mathrm{d}\z_k} 





%\newcommand{\propq}{ \underset{ \text{wrt.}\put(1,1){\scriptsize *}q }{\propto} }

\newcommand{\eqs}[1]{ \stackrel{#1}{=}  }
\newcommand{\eqq}{  \overset{\text{!}}{=} }
\newcommand{\propqz}{ \underset{ \overset{\text{wrt.}}{\qz} }{\propto} }
\newcommand{\propz}{ \underset{ \overset{\text{wrt.}}{\z} }{\propto} }
\newcommand{\propzj}{ \underset{ \overset{\text{wrt.}}{\zj} }{\propto} }
%\newcommand{\constw}{ \underset{ \text{ wrt. \w} }{\text{ const}} }
%\newcommand{\consty}{ \underset{ \text{ wrt. \y} }{\text{ const}} }
\newcommand{\const}[1]{{\underset{ \text{ wrt. #1} }{\text{ const}} }}

\newcommand{\tr}[1]{{#1^\top}}
\newcommand{\inv}[1]{{#1^{-1}}}
\newcommand{\trk}[1]{{(#1)^\top}}
\newcommand{\invk}[1]{{(#1)^{-1}}}
\newcommand{\tin}[1]{{(#1^\top)^{-1}}}

\newcommand{\Xt}{\X^\top}
\newcommand{\xt}{\x^\top}
\newcommand{\yt}{\y^\top}
\newcommand{\wt}{\w^\top}
\newcommand{\w}{w}
\newcommand{\1}{\mathds{1}}



\begin{document}

% header configuration
\title{\b{Exercise Sheet 1}}
\author{Machine Learning 2, SS16}

\maketitle

% authors
Sascha Huk, 321249;\quad Viktor Jeney, 348969;\quad Mario Tambos, ??????;\quad Jan Tinapp, 0380549\\


 
\extitle{Exercise 2}
(i) We have to prove $\wt C\w \eqs{?} \epsilon \eqs{(3)} |x-\sum_jw_j\eta_j|^2	$. \\
\begin{gather*}
	\wt C\w
	\\ 
	= \wt (\1_kx^\top - \eta)(\1_kx^\top - \eta)^\top\w 
	\\
	= \wt (\1_kx^\top - \eta)(x\1_k^\top - \eta^\top)\w 
	\\	
	= \wt\1_kx^\top x\1_k^\top\w - \wt\1_kx^\top \eta^\top\w
	  - \wt\eta x\1_k^\top\w +\wt\eta \eta^\top\w
	\\
	= (\wt\1_kx^\top)(x\1_k^\top\w) - 2(\wt\1_kx^\top)(\eta^\top\w) + (\wt\eta)(\eta^\top\w)
	\\
	= |(\wt\1_kx^\top) - (\wt\eta)|^2
	\\
	= |\wt(\1_kx^\top - \eta)|^2
	\\
	= |\sum_jw_j(x - \eta_j)|^2	
\end{gather*}
Since $\sum_iw_i=1$ we find $\sum_iw_ix=x$, which leads to the desired result.\qed
\\
\\
We now perform Lagrange optimization:  
\begin{gather*}
	\Lambda(w,\lambda) = \wt Cw - \lambda(\wt\mathds{1}_k - 1)
	\\
	{\partial\Lambda \over \partial w} = 2C\w - \lambda\1_k \eqq 0 
	\quad\Longrightarrow\quad 
	2C\w = \lambda\1_k
	\quad\Longrightarrow\quad
	\w = {\lambda \over 2} C^{-1} \1_k
	\\
	{\partial\Lambda \over \partial \lambda} = \wt\1_k - 1 \eqq 0
	\quad\Longrightarrow\quad 
	\wt\1_k = 1 
\end{gather*}
We now replace w in the constraint $\wt\1_k = 1 $. Since $C=C^\top$ we find 
\begin{gather*}
	{\lambda \over 2} = {1\over \1_k^\top C^{-1}\1_k } 
\end{gather*}
(ii) Replacing $\lambda/2$ in the deduced definition of $w$ leads to the desired result. \qed\\
(iii) Multiplying by C from the left in the deduced definition of $w$ leads to $2Cw=\lambda\1_k$. 
W.l.o.g. we focus on the case $\lambda=2$, which leads to the desired result.   \qed
\end{document}