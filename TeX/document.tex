\documentclass[a4paper]{article}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\pagestyle{plain}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{eurosym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{dlfltxbcodetips}
\usepackage{cancel}

%    \usepackage{tkz-graph}  
%    \usetikzlibrary{shapes.geometric}%   
% 	 \usetikzlibrary{bayesnet}
	
\input{preamble.tex} % import config
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}  
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\vivid}{\stackrel{\text{vivid}}{=}}
\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand\indep{\protect\mathpalette{\protect\indeP}{\perp}}
\def\indeP#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\mlq}{F(\q)}
\newcommand{\si}{s}
\newcommand{\sj}{{s'}}
\newcommand{\p}{p}
\newcommand{\q}{r}
\newcommand{\z}{w}
\newcommand{\x}{\mathcal{D}}
%\newcommand{\mlq}{-L(\q)}
%\newcommand{\si}{i}
%\newcommand{\sj}{j}
%\newcommand{\p}{p}
%\newcommand{\q}{q}
%\newcommand{\z}{Z}
%\newcommand{\x}{X}

\newcommand{\pxgz}{\p(\x|\z)}
\newcommand{\pzgx}{\p(\z|\x)}
\newcommand{\pxz}{\p(\x,\z)}
\newcommand{\px}{\p(\x)}

\newcommand{\qz}{\q(\z)}
\newcommand{\qi}{\q_\si}
\newcommand{\qj}{\q_\sj}
\newcommand{\qk}{\q_k}
\newcommand{\pz}{\p(\z)}
\newcommand{\pzi}{\p(\z_\si)}
\newcommand{\pzj}{\p(\z_\sj)}
\newcommand{\pzk}{\p(\z_k)}



\newcommand{\zi}{\z_\si}
\newcommand{\zj}{\z_\sj}
\newcommand{\zk}{\z_k}
\newcommand{\dz}{\mathrm{d}\z}
\newcommand{\dzi}{\mathrm{d}\z_\si}
\newcommand{\dzj}{\mathrm{d}\z_\sj}
\newcommand{\dzk}{\mathrm{d}\z_k} 





%\newcommand{\propq}{ \underset{ \text{wrt.}\put(1,1){\scriptsize *}q }{\propto} }

\newcommand{\eqs}[1]{ \stackrel{#1}{=}  }
\newcommand{\eqq}{  \overset{\text{!}}{=} }
\newcommand{\propqz}{ \underset{ \overset{\text{wrt.}}{\qz} }{\propto} }
\newcommand{\propz}{ \underset{ \overset{\text{wrt.}}{\z} }{\propto} }
\newcommand{\propzj}{ \underset{ \overset{\text{wrt.}}{\zj} }{\propto} }
%\newcommand{\constw}{ \underset{ \text{ wrt. \w} }{\text{ const}} }
%\newcommand{\consty}{ \underset{ \text{ wrt. \y} }{\text{ const}} }
\newcommand{\const}[1]{{\underset{ \text{ wrt. #1} }{\text{ const}} }}

\newcommand{\tr}[1]{{#1^\top}}
\newcommand{\inv}[1]{{#1^{-1}}}
\newcommand{\trk}[1]{{(#1)^\top}}
\newcommand{\invk}[1]{{(#1)^{-1}}}
\newcommand{\tin}[1]{{(#1^\top)^{-1}}}

\newcommand{\Yt}{{Y^\top}}
\newcommand{\Xt}{{X^\top}}
\newcommand{\xt}{{\x^\top}}
\newcommand{\yt}{{\y^\top}}
\newcommand{\wt}{{\w^\top}}
\newcommand{\w}{w}
\newcommand{\lx}{{\lambda_{x}}}
\newcommand{\ly}{{\lambda_{y}}}
\newcommand{\ax}{{\alpha_{x}}}
\newcommand{\ay}{{\alpha_{y}}}

\newcommand{\axt}{\alpha_{x}^\top}
\newcommand{\ayt}{\alpha_{y}^\top}
\newcommand{\Cxx}{C_{xx}}
\newcommand{\Cxy}{C_{xy}}
\newcommand{\Cyx}{C_{yx}}
\newcommand{\Cyy}{C_{yy}}
\newcommand{\1}{\mathds{1}}
\newcommand{\lag}{\mathcal{L}}


\begin{document}

% header configuration
\title{\b{Exercise Sheet 3}}
\author{Machine Learning 2, SS16}

\maketitle

% authors
Sascha Huk, 321249;\quad Viktor Jeney, 348969;\quad Mario Tambos, ??????;\quad Jan Tinapp, 0380549\\


 
\extitle{Exercise 1}
\textbf{(a)}
For data-span constructed $w_x = X\ax$ and $w_y = Y\ay$ the primal problem is:  
%\begin{align*}
%	\argmax_{\ax,\ay} (X\ax)^\top C_{xy} Y\ay \\
%	\text{s.t. } (X\ax)^\top C_{xx} X\ax - 1 &= 0 \\
%	 (Y\ay)^\top C_{yy} Y\ay - 1 &= 0
%\end{align*}
\begin{align*}
	\max_{\ax,\ay} &\ax^\top X^\top C_{xy} Y\ay \\ 
	\text{s.t. } \ax^\top X^\top C_{xx} X\ax - 1 &= 0 
	\quad\quad ,\quad\quad
	 \ay^\top Y^\top C_{yy} Y\ay - 1 = 0
\end{align*}
Lagrangian (the factor 1/2 is introduced just for convenience):
\begin{align*}
	\mathcal{L}=\axt\Xt\Cxy Y\ay - {1\over 2}\lx(\axt\Xt\Cxx X\ax - 1)
	- {1\over 2}\ly(\ayt\Yt\Cyy Y\ay - 1)
\end{align*}
Partial derivatives:
\begin{align*}
	{\partial\lag\over\partial\axt} = \Xt\Cxy Y\ay - \lx\Xt\Cxx X\ax \eqq 0
	\quad\quad ,&\quad\quad
	{\partial\lag\over\partial\ayt} = \Yt\Cyx X\ax - \ly\Yt\Cyy Y\ay \eqq 0
\end{align*}
We now multiply with $\axt$, $\ayt$
\begin{align*}
	\axt\Xt\Cxy Y\ay = \lx\axt\Xt\Cxx X\ax
	\quad\quad ,&\quad\quad
	\ayt\Yt\Cyx X\ax = \ly\ayt\Yt\Cyy Y\ay
	\\
	\Longrightarrow
	\axt\Xt\Cxy Y\ay = \lx\axt\Xt\Cxx X\ax
	\quad\quad ,&\quad\quad
	\axt\Xt\Cxy Y\ay = \ly\ayt\Yt\Cyy Y\ay	
\end{align*}
From the auto-cov constraints follows
\begin{align*}
	&\axt\Xt\Cxy Y\ay = \lx\underbrace{\axt\Xt\Cxx X\ax}_{=1} 
	= \ly\underbrace{\ayt\Yt\Cyy Y\ay}_{=1}
	\quad \Longrightarrow \quad \lx = \ly
\end{align*}
Now the derivatives can be rewritten as follows:
\begin{align*}
	\Xt\Cxy Y\ay \eqq \lx\Xt\Cxx X\ax 
	\quad\quad ,&\quad\quad
	\Yt\Cyx X\ax \eqq \lx\Yt\Cyy Y\ay
\end{align*} 
The same in blockmatrix form:
\begin{align*}
	\begin{bmatrix}
	0 & \Xt\Cxy Y  \\
	\Yt\Cyx X & 0 
	\end{bmatrix}
	\begin{bmatrix}
	 \ax  \\
	 \ay
	\end{bmatrix}
\eqq	 
	 \lx		
	\begin{bmatrix}
	\Xt\Cxx X & 0 \\
	0 & \Yt\Cyy Y  
	\end{bmatrix}
	\begin{bmatrix}
	 \ax  \\
	 \ay
	\end{bmatrix}
\\
\Longrightarrow
	\begin{bmatrix}
	\Xt\Cxx X & 0 \\
	0 & \Yt\Cyy Y  
	\end{bmatrix}^{-1}
	\begin{bmatrix}
	0 & \Xt\Cxy Y  \\
	\Yt\Cyx X & 0 
	\end{bmatrix}
	\begin{bmatrix}
	 \ax  \\
	 \ay
	\end{bmatrix}
\eqq	 
	 \lx		
	I
	\begin{bmatrix}
	 \ax  \\
	 \ay
	\end{bmatrix}
\end{align*}
\textbf{(a)} $\Xt\Cxx X$ and $\Yt\Cxx Y$ are positive semi-definite. 
At least after regularizing one of these blocks, the diagonal block matrix
becomes positive definite / invertible, which leads to a non-trivial 
solution. \\
\textbf{(b)} In comparison, the same conditions occur when viewing the Jacobian of $\lag$. 
The Jacobian has to be negative definite. 
Since the Jacobian is symmetric, this is true iff
the determinants of the principle minors alternate. 
We already know that the 
first principle minor $\Xt\Cxx X$ or $\Yt\Cyy Y$ can only be positive. 
Then, the second principle minor
should be negative, which means $-A^2-B^2-AB-BA<0$. 
This is true as this is the quadratic form $-(A-B)^2<0$. 
Ultimately, positive 
$\Xt\Cxx X$ or $\Yt\Cyy Y$ is solely necessary for a solution likewise.\\
\textbf{(c)} By finding the solutions $\ax^*$ and $\ay^*$, the dual variable $\lx$
is identified as this is an eigenvalue problem. 
Each eigenvalue $\lx$ corresponds to an eigenvector $[\ax,\ay]^\top$. 
Therefore, the lagrangian does not depend on $\lx$ which means 
$\forall\lx . \lag(\ax^*,\ay^*,\lx) = \lag(\ax^*,\ay^*)$. 
For the dual problem we therefore find
$ 
\min_\lx \max_{\ax,\ay} \lag(\ax,\ay,\lx) 
= \min_\lx \lag(\ax^*,\ay^*,\lx) 
= \lag(\ax^*,\ay^*)
$
\newpage
\extitle{Exercise 2}
\textbf{(a)} Let $\Phi$ be a general symbol for an appropriate feature mapping and 
define $\Phi(X) := [\Phi(x^{(1)}, \Phi(x^{(2)}), \ldots, \Phi(x^{(N)}]$ for each 
dataset X. As in exercise (1), by starting with $w_x = \Phi(X)\ax$ and 
$w_y = \Phi(Y)\ay$ one ends up with the eigenvalue problem stated in the task 
description of exercise (1b), where $A = \Phi(X)^\top \Phi(X) = K_x$ and
$B = \Phi(X)^\top \Phi(X) = K_y$. The inner products in the Gramian matrices 
don't need to be computed via $\Phi$ explicitly but rather via the kernels' 
definitions. \\
\\
\textbf{(b)} 
The results $\ax, \ay$ are linear combinations of the N vectors in the respective 
feature spaces, which is spanned by $\Phi(X)$, $\Phi(Y)$. The solutions can be interpreted as directions in the input space 
if $\Phi$ is a conformal map, otherwise not.    

\end{document}









