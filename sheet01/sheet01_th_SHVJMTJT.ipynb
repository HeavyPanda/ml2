{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 1\n",
    "## Machine Learning 2, SS16\n",
    "#### April 30, 2016\n",
    "#### Sascha Huk, 321249; Viktor Jeney, 348969; Mario Tambos, 380599; Jan Tinapp, 380549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "####  (i)\n",
    "Given the following problem:\n",
    "$$\n",
    "\\min_{w} {E(w) = \\sum_i {\\left|\\vec{x}_i-\\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "\n",
    "we are trying to prove that the multiplication of each vector $\\vec{x}_i$ by a constant scalar $\\alpha \\in \\mathbb{R}^+ \\setminus \\{0\\}$ does not alter the problem's solution.\n",
    "\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|\\alpha \\vec{x}_i-\\sum_j{w_{ij} \\alpha \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|\\alpha \\vec{x}_i-\\alpha \\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\alpha^2 \\left| \\vec{x}_i - \\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} \\alpha^2 {\\sum_i {\\left| \\vec{x}_i - \\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "\n",
    "Since the multiplication by $\\alpha^2 \\in \\mathbb{R}^+ \\setminus \\{0\\}$ doesn't change the minima with respect to $w$, the minimizaiton problem remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii)\n",
    "Given the following problem:\n",
    "$$\n",
    "\\min_{w} {E(w) = \\sum_i {\\left|\\vec{x}_i-\\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "\n",
    "we are trying to prove that the addition of constant vector $\\vec{v} \\in \\mathbb{R}^D$ to each vector $\\vec{x}_i$ does not alter the problem's solution.\n",
    "\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|(x_i+\\vec{v})-\\sum_j{w_{ij} (x_j+\\vec{v})}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|x_i+\\vec{v} - \\left(\\sum_j{w_{ij} x_j + w_{ij} \\vec{v}}\\right)\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|x_i+\\vec{v} - \\left(\\sum_j{w_{ij} x_j}\\right) - \\left(\\sum_j{w_{ij} \\vec{v}}\\right)\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\stackrel{\\text{minimization constraint}}{\\equiv}\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|x_i+\\vec{v} - \\left(\\sum_j{w_{ij} x_j}\\right) - \\vec{v}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|x_i - \\sum_j{w_{ij} x_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (iii)\n",
    "Given the following problem:\n",
    "$$\n",
    "\\min_{w} {E(w) = \\sum_i {\\left|\\vec{x}_i-\\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "\n",
    "we are trying to prove that the multiplication of each vector $\\vec{x}_i$ by a constant, orthogonal $D \\times D$ matrix $U$ does not alter the problem's solution.\n",
    "\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|U \\vec{x}_i-\\sum_j{w_{ij} U \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|U \\vec{x}_i-\\sum_j{U w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left|U \\vec{x}_i - U \\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left| U \\left(\\vec{x}_i - \\sum_j{w_{ij} \\vec{x}_j}\\right)\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$\n",
    "\n",
    "Since for all orthogonal matrices $U\\in \\mathbb{R}^{D \\times D}$ and vectors $\\vec{x} \\in \\mathbb{R}^D$ we have that $\\left|U\\vec{x}\\right| = \\left|\\vec{x}\\right|$\n",
    "\n",
    "$$\n",
    "\\equiv\n",
    "$$\n",
    "$$\n",
    "\\min_{w} {\\sum_i {\\left| \\vec{x}_i - \\sum_j{w_{ij} \\vec{x}_j}\\right|^2}}\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_j{w_{ij}} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "#### (i)\n",
    "\n",
    "$$\n",
    "\\newcommand{\\eqs}[1]{\\stackrel{#1}{=}}\n",
    "\\newcommand{\\eqq}{\\overset{\\text{!}}{=}}\n",
    "\\newcommand{\\Xt}{\\X^\\top}\n",
    "\\newcommand{\\xt}{\\x^\\top}\n",
    "\\newcommand{\\yt}{\\y^\\top}\n",
    "\\newcommand{\\wt}{\\w^\\top}\n",
    "\\newcommand{\\w}{w}\n",
    "\\newcommand{\\1}{\\mathbb{1}}\n",
    "$$\n",
    "We have to prove $\\wt C\\w \\eqs{?} \\epsilon \\eqs{(3)} |x-\\sum_jw_j\\eta_j|^2\t$.\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\t\\wt C\\w\n",
    "\t\\\\ \n",
    "\t= \\wt (\\1_kx^\\top - \\eta)(\\1_kx^\\top - \\eta)^\\top\\w \n",
    "\t\\\\\n",
    "\t= \\wt (\\1_kx^\\top - \\eta)(x\\1_k^\\top - \\eta^\\top)\\w \n",
    "\t\\\\\t\n",
    "\t= \\wt\\1_kx^\\top x\\1_k^\\top\\w - \\wt\\1_kx^\\top \\eta^\\top\\w\n",
    "\t  - \\wt\\eta x\\1_k^\\top\\w +\\wt\\eta \\eta^\\top\\w\n",
    "\t\\\\\n",
    "\t= (\\wt\\1_kx^\\top)(x\\1_k^\\top\\w) - 2(\\wt\\1_kx^\\top)(\\eta^\\top\\w) + (\\wt\\eta)(\\eta^\\top\\w)\n",
    "\t\\\\\n",
    "\t= |(\\wt\\1_kx^\\top) - (\\wt\\eta)|^2\n",
    "\t\\\\\n",
    "\t= |\\wt(\\1_kx^\\top - \\eta)|^2\n",
    "\t\\\\\n",
    "\t= |\\sum_jw_j(x - \\eta_j)|^2\t\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "Since $\\sum_iw_i=1$ we find $\\sum_iw_ix=x$, which leads to the desired result. $\\Box$\n",
    "\n",
    "We now perform Lagrange optimization:  \n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\t\\Lambda(w,\\lambda) = \\wt Cw - \\lambda(\\wt\\1_k - 1)\n",
    "\t\\\\\n",
    "\t{\\partial\\Lambda \\over \\partial w} = 2C\\w - \\lambda\\1_k \\eqq 0 \n",
    "\t\\quad\\Longrightarrow\\quad \n",
    "\t2C\\w = \\lambda\\1_k\n",
    "\t\\quad\\Longrightarrow\\quad\n",
    "\t\\w = {\\lambda \\over 2} C^{-1} \\1_k\n",
    "\t\\\\\n",
    "\t{\\partial\\Lambda \\over \\partial \\lambda} = \\wt\\1_k - 1 \\eqq 0\n",
    "\t\\quad\\Longrightarrow\\quad \n",
    "\t\\wt\\1_k = 1 \n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "We now replace w in the constraint $\\wt\\1_k = 1 $. Since $C=C^\\top$ we find \n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\t{\\lambda \\over 2} = {1\\over \\1_k^\\top C^{-1}\\1_k } \n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii)\n",
    "\n",
    "Replacing $\\lambda/2$ in the deduced definition of $w$ leads to the desired result. \n",
    "The candidate w is indeed a minimum since ${\\partial^2\\Lambda\\over\\partial{w}^2}=2C$ \n",
    "(invertible covariance matrices have positive definite quadratic forms). $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii)\n",
    "\n",
    "Multiplying by $C$ from the left in the deduced definition of $w$ leads to $Cw={\\lambda\\over 2}\\1_k$ (resp. $Cw'=\\1_k$ for\n",
    "$w'={2\\over\\lambda}w$). Both, $w'$ and $w$, point to the same direction. \n",
    "Therefore, by rescaling $w'$ such that ${w'}^\\top\\1_k=1={w}^\\top\\1_k$ the desired $w$ is identified.$\\Box$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}